{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Installing necessary libraries:\n!pip install -U \"transformers>=4.39.0\" --q\n!pip install --q peft bitsandbytes\n!pip install --q accelerate\n!pip install -U \"trl>=0.8.3\" --q\n!pip install --q datasets\n!pip install --upgrade huggingface_hub\n!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:37:01.104298Z","iopub.execute_input":"2024-07-14T05:37:01.104662Z","iopub.status.idle":"2024-07-14T05:39:04.341575Z","shell.execute_reply.started":"2024-07-14T05:37:01.104631Z","shell.execute_reply":"2024-07-14T05:39:04.340587Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.7.4)\nCollecting flash-attn\n  Downloading flash_attn-2.6.1.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.1.2)\nCollecting einops (from flash-attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.1-cp310-cp310-linux_x86_64.whl size=198130327 sha256=2e7a4ae16d20c1c7981f7a9f9a1833b9b528997b990d97c4a6d6d38d3574c773\n  Stored in directory: /root/.cache/pip/wheels/91/6a/38/f0faa036b4ac73a73247386f1ab1bb4cb4f6e72e6861a779f1\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.8.0 flash-attn-2.6.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tuning Vision Language Model -> LLaVA model on `llava-instruct-mix-vsft`:","metadata":{}},{"cell_type":"code","source":"! nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:39:15.030448Z","iopub.execute_input":"2024-07-14T05:39:15.031073Z","iopub.status.idle":"2024-07-14T05:39:16.079379Z","shell.execute_reply.started":"2024-07-14T05:39:15.031028Z","shell.execute_reply":"2024-07-14T05:39:16.078428Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sun Jul 14 05:39:15 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0             31W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"#Necesary imports:\nimport torch\nfrom transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:39:18.125376Z","iopub.execute_input":"2024-07-14T05:39:18.126185Z","iopub.status.idle":"2024-07-14T05:39:35.740009Z","shell.execute_reply.started":"2024-07-14T05:39:18.126150Z","shell.execute_reply":"2024-07-14T05:39:35.739204Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-14 05:39:23.006237: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-14 05:39:23.006343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-14 05:39:23.123428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading up the model in 4-bit quantized version:","metadata":{}},{"cell_type":"code","source":"#Selecting which model to use:\nmodel_card = \"bczhou/tiny-llava-v1-hf\"\n\n#Configuring the quantization of the model:\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,)\n\n#Setting & Loading up the model:\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_card, \n    quantization_config=quantization_config, \n    torch_dtype=torch.float16\n)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:39:53.240612Z","iopub.execute_input":"2024-07-14T05:39:53.241790Z","iopub.status.idle":"2024-07-14T05:41:39.119783Z","shell.execute_reply.started":"2024-07-14T05:39:53.241755Z","shell.execute_reply":"2024-07-14T05:41:39.118761Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc8777c726d74ffd9405d2316990a6cc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n  warnings.warn(\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/61.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb508e9d17c4270bfe9e017a4557407"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ae56824e8014aa48038c3907c9af882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a128c7ce59b4bfc98045686b4f0100e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/661M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7dc5aba9aa46039ce6d6974f1f9d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df9de7f5a044fca98b3f5f69e0c0888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6d4147bee24aee8408f992aa8fef44"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:41:49.837806Z","iopub.execute_input":"2024-07-14T05:41:49.838520Z","iopub.status.idle":"2024-07-14T05:41:49.849530Z","shell.execute_reply.started":"2024-07-14T05:41:49.838487Z","shell.execute_reply":"2024-07-14T05:41:49.848598Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LlavaForConditionalGeneration(\n  (vision_tower): CLIPVisionModel(\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n        (position_embedding): Embedding(577, 1024)\n      )\n      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-23): 24 x CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (multi_modal_projector): LlavaMultiModalProjector(\n    (linear_1): Linear4bit(in_features=1024, out_features=2048, bias=True)\n    (act): GELUActivation()\n    (linear_2): Linear4bit(in_features=2048, out_features=2048, bias=True)\n  )\n  (language_model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(32064, 2048)\n      (layers): ModuleList(\n        (0-21): 22 x LlamaDecoderLayer(\n          (self_attn): LlamaSdpaAttention(\n            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n            (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n            (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n            (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n            (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm()\n          (post_attention_layernorm): LlamaRMSNorm()\n        )\n      )\n      (norm): LlamaRMSNorm()\n    )\n    (lm_head): Linear(in_features=2048, out_features=32064, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2. Creating a base chat template & Initiating LLaVA processor:","metadata":{}},{"cell_type":"code","source":"BASE_CHAT_TEMPLATE = \"\"\"\n                      A chat between a curious user and an artificial intelligence assistant. \\\n                      The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n                      {% for message in messages %}{% if message['role'] == 'user' %}\\\n                      USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}\\\n                      {% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\"\n                     ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:41:57.637525Z","iopub.execute_input":"2024-07-14T05:41:57.637912Z","iopub.status.idle":"2024-07-14T05:41:57.643309Z","shell.execute_reply.started":"2024-07-14T05:41:57.637882Z","shell.execute_reply":"2024-07-14T05:41:57.642157Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_card)\ntokenizer.chat_template = BASE_CHAT_TEMPLATE\nprocessor = AutoProcessor.from_pretrained(model_card)\nprocessor.tokenizer = tokenizer ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:41:58.645534Z","iopub.execute_input":"2024-07-14T05:41:58.645868Z","iopub.status.idle":"2024-07-14T05:42:00.890007Z","shell.execute_reply.started":"2024-07-14T05:41:58.645843Z","shell.execute_reply":"2024-07-14T05:42:00.889096Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59df62f452054100bb3877958133d939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4242794b73bc4d18ab91889f95dfff24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824e0d71dc4e4b09bb16b84adaf6d2a2"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba4be00c86549f59e3063b18e4145fe"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Loading Up the dataset:","metadata":{}},{"cell_type":"markdown","source":"### Creating a DataCollator for processing the data:","metadata":{}},{"cell_type":"code","source":"class LlavaDataCollator:\n    def __init__(self, processor):\n        self.processor = processor\n        \n    def __call__(self, examples):\n        texts = []  # -> Creating empty lists to later store the texts seperately\n        images = [] # -> Creating an empty list to later store the images seperately\n        \n        for example in examples:\n            messages = example[\"messages\"]\n            text = self.processor.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=False)\n            texts.append(text)\n            \n            images.append(example[\"images\"][0])\n            \n            batch = self.processor(texts, images, return_tensors='pt', padding=True)\n            labels = batch[\"input_ids\"].clone()\n            if self.processor.tokenizer.pad_token_id is not None:\n                labels[labels == self.processor.tokenizer.pad_token_id] = -100\n            \n            batch[\"labels\"] = labels\n            return batch\n        \ndata_collator = LlavaDataCollator(processor)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:42:02.992448Z","iopub.execute_input":"2024-07-14T05:42:02.993286Z","iopub.status.idle":"2024-07-14T05:42:03.000992Z","shell.execute_reply.started":"2024-07-14T05:42:02.993248Z","shell.execute_reply":"2024-07-14T05:42:03.000110Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the dataset:","metadata":{}},{"cell_type":"code","source":"raw_data = load_dataset('HuggingFaceH4/llava-instruct-mix-vsft')\ntrain_data = raw_data[\"train\"]\nevaluate_data = raw_data[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:42:05.230826Z","iopub.execute_input":"2024-07-14T05:42:05.231876Z","iopub.status.idle":"2024-07-14T05:45:43.632945Z","shell.execute_reply.started":"2024-07-14T05:42:05.231840Z","shell.execute_reply":"2024-07-14T05:45:43.631971Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/868 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42de42ab81404a138b0471966bf2fe15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b63f623fa1a4c188d7c1899dc2416fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc779ae1971640759fe61c9ae82668b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/20 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e58ba002528479f897e5939eff674ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/285M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d0d23ade9746b9b6e26635b0b99ac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/284M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea902ae5c4b43db87c153ebbbe6bec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/259155 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e941e84a9a824ced8e65e21aaa4221b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/13640 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f49bab08164982b329dccb45304d4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f28ff6e7c554b9c885772d12db4834a"}},"metadata":{}}]},{"cell_type":"code","source":"#Printing the length of training and evaluation dataset:\nprint(\"Length of Training Split: \", len(train_data))\nprint(\"Length of Evaluation Split: \", len(evaluate_data))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:45:50.221191Z","iopub.execute_input":"2024-07-14T05:45:50.221555Z","iopub.status.idle":"2024-07-14T05:45:50.231754Z","shell.execute_reply.started":"2024-07-14T05:45:50.221526Z","shell.execute_reply":"2024-07-14T05:45:50.230841Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Length of Training Split:  259155\nLength of Evaluation Split:  13640\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fine-Tuning Tiny-LLaVA model(v1.0):","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T05:45:52.812150Z","iopub.execute_input":"2024-07-14T05:45:52.812516Z","iopub.status.idle":"2024-07-14T05:46:01.820716Z","shell.execute_reply.started":"2024-07-14T05:45:52.812487Z","shell.execute_reply":"2024-07-14T05:46:01.819788Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbdf88eea6649fbb14e0bde15d2606e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### a. Setting up the training arguments:","metadata":{}},{"cell_type":"code","source":"train_args = TrainingArguments(\n    output_dir=\"tiny-llava-v1-finetuned-mix-vsft\",\n    report_to=\"tensorboard\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    logging_steps=5,\n    num_train_epochs=1,\n    max_steps=500,\n    push_to_hub=True,\n    gradient_checkpointing=True,\n    remove_unused_columns=False,\n    fp16=True,\n    bf16=False\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T06:20:26.146947Z","iopub.execute_input":"2024-07-14T06:20:26.147640Z","iopub.status.idle":"2024-07-14T06:20:26.175595Z","shell.execute_reply.started":"2024-07-14T06:20:26.147608Z","shell.execute_reply":"2024-07-14T06:20:26.174808Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### b. Setting up LoRA configuration:","metadata":{}},{"cell_type":"code","source":"lora_configure = LoraConfig(\n    r=64,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T06:20:27.982527Z","iopub.execute_input":"2024-07-14T06:20:27.982991Z","iopub.status.idle":"2024-07-14T06:20:27.987536Z","shell.execute_reply.started":"2024-07-14T06:20:27.982958Z","shell.execute_reply":"2024-07-14T06:20:27.986584Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### c. Creating the Supervised Fine-Tuning Trainer(SFTTrainer) object:","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=train_args,\n    train_dataset=train_data,\n    eval_dataset=evaluate_data,\n    peft_config=lora_configure,\n    dataset_text_field=\"text\", #->dummy field\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    dataset_kwargs={\"skip_prepare_dataset\":True}\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T06:20:29.222897Z","iopub.execute_input":"2024-07-14T06:20:29.223272Z","iopub.status.idle":"2024-07-14T06:20:29.708859Z","shell.execute_reply.started":"2024-07-14T06:20:29.223242Z","shell.execute_reply":"2024-07-14T06:20:29.707970Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, dataset_kwargs. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:366: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### d. Loading up tensorboard for logging:","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/tiny-llava-v1-finetuned-mix-vsft","metadata":{"execution":{"iopub.status.busy":"2024-07-14T06:20:31.605209Z","iopub.execute_input":"2024-07-14T06:20:31.605558Z","iopub.status.idle":"2024-07-14T06:20:36.625850Z","shell.execute_reply.started":"2024-07-14T06:20:31.605532Z","shell.execute_reply":"2024-07-14T06:20:36.624875Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"### e. Start the finetuning of the model:","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T06:20:36.627412Z","iopub.execute_input":"2024-07-14T06:20:36.627761Z","iopub.status.idle":"2024-07-14T06:30:44.334431Z","shell.execute_reply.started":"2024-07-14T06:20:36.627735Z","shell.execute_reply":"2024-07-14T06:30:44.333543Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 10:01, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.041700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.182300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.057400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.661400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.038600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.936500</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.836100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.952300</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.603600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.673400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.643500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.819400</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.717000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.780500</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.684000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.627500</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.663400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.722000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.611500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.677900</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.476000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.532600</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.621800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.513900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.331500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.390400</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.458100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.319600</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.670300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.260500</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.398700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.305300</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.397900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.183900</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.455300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.633500</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.528300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.337800</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.127900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.316100</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.989400</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.622700</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>1.029600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.080700</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.245800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.022100</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.889300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.942500</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>1.089900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.945600</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.950300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.891700</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.912600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.934100</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.087400</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.053300</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.948900</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.868600</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.885900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.779100</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.979300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.285700</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.933400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.836100</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.721800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.883200</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.875800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.715400</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.811600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.706400</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.937600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.845600</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.683900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.759900</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.660700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.771300</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.843100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.708400</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.815100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.864100</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.865400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.837400</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.573300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.823000</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.748200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.777700</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.752400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.947800</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.791500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.512400</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.691300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.772600</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.796600</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.070400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.738600</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.823600</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>0.714700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.860700</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>0.805700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.673500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n  warnings.warn(\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.1657405271530152, metrics={'train_runtime': 602.4026, 'train_samples_per_second': 6.64, 'train_steps_per_second': 0.83, 'total_flos': 976905881266176.0, 'train_loss': 1.1657405271530152, 'epoch': 0.01543448062972681})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T07:01:40.842143Z","iopub.execute_input":"2024-07-14T07:01:40.842782Z","iopub.status.idle":"2024-07-14T07:01:42.991955Z","shell.execute_reply.started":"2024-07-14T07:01:40.842749Z","shell.execute_reply":"2024-07-14T07:01:42.990984Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:140: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n  warnings.warn(\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/sid29/tiny-llava-v1-finetuned-mix-vsft/commit/41cb03a700ab2a0dae6e2d75fff8fe04d276eae6', commit_message='End of training', commit_description='', oid='41cb03a700ab2a0dae6e2d75fff8fe04d276eae6', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}